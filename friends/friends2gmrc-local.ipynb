{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitgrmced8b2bf1a9cd43cdaea26a31a3a7e50b",
   "display_name": "Python 3.7.9 64-bit ('grmc')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Download necessary stuff"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Download the annotations and stuff\n",
    "import os\n",
    "os.makedirs('DEBUG', exist_ok=True)\n",
    "\n",
    "if not os.path.isfile('DEBUG/all-done'):\n",
    "    %cd DEBUG\n",
    "    !git clone https://github.com/leolani/cltl-face-all\n",
    "    !wget https://raw.githubusercontent.com/leolani/cltl-face-all/master/examples/smaller-datasets-jsons/datasets.json\n",
    "\n",
    "    !gdown --id 1rsLbfgQYztDtrPFqEmkh-2d_0ap1qd_s\n",
    "    !unzip visual-features-smaller-dataset.zip\n",
    "    !rm visual-features-smaller-dataset.zip\n",
    "\n",
    "    !gdown --id 16ck7plW9v9eSHGCs5wuB2AhhufPRt3Wi\n",
    "    !unzip smaller-dataset.zip\n",
    "    !rm smaller-dataset.zip\n",
    "    !touch all-done\n",
    "    %cd .."
   ]
  },
  {
   "source": [
    "## Read pre-computed visual features and annotations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import av\n",
    "import cv2\n",
    "import random\n",
    "from glob import glob\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "VISUAL_FEATURES_PATH = 'DEBUG/visual-features-smaller-dataset/'\n",
    "VIDEOS_PATH = \"DEBUG/smaller-dataset/\"\n",
    "\n",
    "with open('DEBUG/datasets.json', 'r') as stream:\n",
    "    datasets = json.load(stream)\n",
    "\n",
    "datasets = datasets['large']\n",
    "\n",
    "visual_features = glob('DEBUG/visual-features-smaller-dataset/*.npy')\n",
    "visual_features = {os.path.basename(vf).split('.npy')[0] : np.load(vf, allow_pickle=True).item() for vf in visual_features}\n",
    "\n",
    "with open('friends-time/friends-time.pkl', 'rb') as stream:\n",
    "    friends_time = pickle.load(stream)"
   ]
  },
  {
   "source": [
    "## Run on the images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import av\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import csv\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import shutil\n",
    "import uuid\n",
    "\n",
    "THRESHOLDS = {'face': 0.8, 'angle': 1.15}\n",
    "\n",
    "predefined_faces = {}\n",
    "for path in glob(os.path.join('DEBUG/cltl-face-all/your-faces/*/*.npy')):\n",
    "    name = path.split('/')[-2]\n",
    "    predefined_faces[name] = np.load(path)\n",
    "\n",
    "def calc_angle_distance(emb1, emb2):\n",
    "    \"\"\"Calculate the angle (radian) distance between the embeddings.\"\"\"\n",
    "    return np.arccos(np.clip((emb1 @ emb2.T), -1, 1))\n",
    "\n",
    "def get_dias(list_of_diautts):\n",
    "    return sorted(list(set([diautt.split('_')[0] for diautt in list_of_diautts])))\n",
    "\n",
    "def get_time_unix_ms(time_string):\n",
    "    hours, minutes, seconds = time_string.split(':')\n",
    "    seconds, milliseconds = seconds.split(',')\n",
    "    hours, minutes, seconds, milliseconds = int(hours), int(minutes), int(seconds), int(milliseconds)\n",
    "    time_datetime = friends_time[season][episode] + timedelta(hours=hours, minutes=minutes, seconds=seconds)\n",
    "    time_unix = time.mktime(time_datetime.timetuple())\n",
    "    time_unix_ms = int(time_unix*1000 + milliseconds)    \n",
    "\n",
    "    return time_unix_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for DATASET in tqdm(['train', 'dev', 'test']):\n",
    "    dataset = datasets[DATASET]\n",
    "    diautts_ = list(dataset.keys())\n",
    "    \n",
    "    dias = get_dias(diautts_)\n",
    "    diautts_ = {dia: [diautt for diautt in diautts_ if dia in diautt] for dia in dias}\n",
    "\n",
    "    for dia, diautts in tqdm(diautts_.items()):\n",
    "        shutil.rmtree(os.path.join('DEBUG/data', dia), ignore_errors=True)\n",
    "        os.makedirs(os.path.join('DEBUG/data', dia, 'image'), exist_ok=True)\n",
    "        os.makedirs(os.path.join('DEBUG/data', dia, 'text'), exist_ok=True)\n",
    "        os.makedirs(os.path.join('DEBUG/data', dia, 'audio'), exist_ok=True)\n",
    "\n",
    "        image_gmrc = []\n",
    "        chat = []\n",
    "        for diautt in tqdm(diautts):\n",
    "            annot = datasets[DATASET][diautt] \n",
    "            vis = visual_features[diautt]\n",
    "            vidpath = os.path.join(VIDEOS_PATH, diautt) + '.mp4'\n",
    "\n",
    "            season = annot['Season']\n",
    "            episode = annot['Episode']\n",
    "            emotion = annot['Emotion']\n",
    "            sentiment = annot['Sentiment']\n",
    "            utterance = annot['Utterance']\n",
    "            speaker = annot['Speaker']\n",
    "\n",
    "            starttime = annot['StartTime']\n",
    "            endtime = annot['EndTime']\n",
    "\n",
    "            time_unix_ms_start = get_time_unix_ms(starttime)\n",
    "            time_unix_ms_end = get_time_unix_ms(endtime)\n",
    "            chat.append([speaker, utterance, time_unix_ms_start])\n",
    "\n",
    "            aud = diautt + '.wav'\n",
    "            !ffmpeg -i $vidpath -q:a 0 -map a DEBUG/data/$dia/audio/$aud\n",
    "\n",
    "            container = av.open(vidpath)\n",
    "            fps = float(container.streams.video[0].average_rate)\n",
    "            spf = 1/fps \n",
    "            mspf = round(spf * 1000)\n",
    "            for i, frame in enumerate(container.decode(video=0)):\n",
    "                idx = frame.index\n",
    "                numpy_RGB = np.array(frame.to_image())\n",
    "                numpy_BGR = cv2.cvtColor(numpy_RGB, cv2.COLOR_RGB2BGR)\n",
    "                img_time = idx*mspf + time_unix_ms_start\n",
    "                impath = os.path.join('DEBUG/data', \n",
    "                                       dia, \n",
    "                                       'image', \n",
    "                                       diautt + f'_frame{str(idx).zfill(5)}_{str(img_time)}.jpg')\n",
    "                cv2.imwrite(impath, numpy_BGR)\n",
    "\n",
    "                features = vis[idx]\n",
    "                # Assume that there is only one unique face per frame.\n",
    "                for feat in features:\n",
    "\n",
    "                    age = round(float(feat['age']), 3)\n",
    "                    gender = round(float(feat['gender']), 3)\n",
    "                    bbox = feat['bbox']\n",
    "                    bbox, faceprob = [int(round(bb)) for bb in bbox[:4]], float(bbox[-1])\n",
    "                    faceprob = round(faceprob, 3)\n",
    "                    embedding = feat['embedding']\n",
    "                    landmark = feat['landmark']\n",
    "\n",
    "                    if faceprob < THRESHOLDS['face']:\n",
    "                        continue\n",
    "\n",
    "                    if speaker not in list(predefined_faces.keys()):\n",
    "                        continue\n",
    "\n",
    "                    embedding.reshape(1, 512)\n",
    "\n",
    "                    dists = {key: calc_angle_distance(embedding, val) for key, val \\\n",
    "                                in predefined_faces.items()}\n",
    "\n",
    "                    if dists[speaker] < THRESHOLDS['angle']:\n",
    "                        to_append = {}\n",
    "                        to_append['array'] = None\n",
    "                        to_append['bounds'] = [0, 0, numpy_BGR.shape[1], numpy_BGR.shape[0]]\n",
    "                        to_append['files'] = [os.path.join('image', os.path.basename(impath))]\n",
    "                        container_id = str(uuid.uuid4())\n",
    "                        to_append['id'] = container_id\n",
    "                        annotations = [\n",
    "                            {\n",
    "                                'source': 'machine',\n",
    "                                'timestamp': round(time.time()*1000),\n",
    "                                'type': 'person',\n",
    "                                'value': \n",
    "                                    {'name': speaker,\n",
    "                                     'age': age,\n",
    "                                     'gender': gender,\n",
    "                                     'faceprob': faceprob}\n",
    "                            }\n",
    "                        ]\n",
    "                        mention_id = str(uuid.uuid4())\n",
    "                        segment = [\n",
    "                            {\n",
    "                                'bounds': bbox,\n",
    "                                'container_id': container_id,\n",
    "                                'type': 'MultiIndex'\n",
    "                                \n",
    "                            }\n",
    "                        ]\n",
    "                        to_append['mentions'] = [\n",
    "                            {\n",
    "                                'annotations': annotations,\n",
    "                                'id': mention_id,\n",
    "                                'segment': segment\n",
    "                            }\n",
    "                        ]\n",
    "                        to_append['modality'] = 'image'\n",
    "                        to_append['ruler'] = {\n",
    "                            'bounds': [0, 0, numpy_BGR.shape[1], numpy_BGR.shape[0]],\n",
    "                            'container_id': container_id,\n",
    "                            'type': 'MultiIndex'\n",
    "                        }\n",
    "                        to_append['time'] = {\n",
    "                            'container_id': container_id,\n",
    "                            'start': time_unix_ms_start,\n",
    "                            'end': time_unix_ms_end,\n",
    "                            'type': 'TemporalRuler',\n",
    "                        }\n",
    "                        to_append['type'] = 'ImageSignal'\n",
    "\n",
    "                        image_gmrc.append(to_append)\n",
    "\n",
    "        with open(os.path.join('DEBUG/data', dia, 'text', f'{dia}.csv'), 'w') as stream:\n",
    "            stream.write('speaker,utterance,time,emotion\\n')\n",
    "\n",
    "            for line in chat:\n",
    "                speaker, utterance, time_unix_ms_start = line\n",
    "                stream.write(speaker)\n",
    "                stream.write(',')\n",
    "                stream.write(f\"\\\"{utterance}\\\"\")\n",
    "                stream.write(',')\n",
    "                stream.write(str(time_unix_ms_start))\n",
    "                stream.write(',')\n",
    "                stream.write(emotion)\n",
    "                stream.write('\\n')    \n",
    "\n",
    "        with open(os.path.join('DEBUG/data', dia, 'image.json'), 'w') as stream:\n",
    "            json.dump(image_gmrc, stream)"
   ]
  }
 ]
}