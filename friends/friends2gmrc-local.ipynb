{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitgrmced8b2bf1a9cd43cdaea26a31a3a7e50b",
   "display_name": "Python 3.7.9 64-bit ('grmc')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Download necessary stuff"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the annotations and stuff\n",
    "\n",
    "!git clone https://github.com/leolani/cltl-face-all\n",
    "\n",
    "!wget https://raw.githubusercontent.com/leolani/cltl-face-all/master/examples/smaller-datatsets-jsons/datasets.json\n",
    "\n",
    "\n",
    "!gdown --id 1-2LeHC_5Cm2gWWT6vBrVhp8jorbjkN1_\n",
    "!unzip visual-features.zip\n",
    "!rm visual-features.zip\n",
    "\n",
    "!gdown --id 16ck7plW9v9eSHGCs5wuB2AhhufPRt3Wi\n",
    "!unzip smaller-dataset.zip\n",
    "!rm smaller-dataset.zip"
   ]
  },
  {
   "source": [
    "## Read pre-computed visual features and annotations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import av\n",
    "import cv2\n",
    "import random\n",
    "from glob import glob\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "VISUAL_FEATURES_PATH = 'visual-features/'\n",
    "VIDEOS_PATH = \"smaller-dataset/\"\n",
    "\n",
    "\n",
    "with open('datasets.json', 'r') as stream:\n",
    "    datasets = json.load(stream)\n",
    "\n",
    "datasets = datasets['large']\n",
    "\n",
    "visual_features = glob('visual-features/*.npy')\n",
    "visual_features = {os.path.basename(vf).split('.npy')[0] : np.load(vf, allow_pickle=True).item() for vf in visual_features}\n",
    "\n",
    "with open('friends-time/friends-time.pkl', 'rb') as stream:\n",
    "    friends_time = pickle.load(stream)"
   ]
  },
  {
   "source": [
    "## Run on the images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import av\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import csv\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "THRESHOLDS = {'face': 0.8, 'angle': 1.15}\n",
    "\n",
    "predefined_faces = {}\n",
    "for path in glob(os.path.join('./cltl-face-all/your-faces/*/*.npy')):\n",
    "    name = path.split('/')[-2]\n",
    "    predefined_faces[name] = np.load(path)\n",
    "\n",
    "def calc_angle_distance(emb1, emb2):\n",
    "    \"\"\"Calculate the angle (radian) distance between the embeddings.\"\"\"\n",
    "    return np.arccos(np.clip((emb1 @ emb2.T), -1, 1))\n",
    "\n",
    "def get_dias(list_of_diautts):\n",
    "    return sorted(list(set([diautt.split('_')[0] for diautt in list_of_diautts])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for DATASET in tqdm(['train', 'dev', 'test']):\n",
    "    dataset = datasets[DATASET]\n",
    "    diautts_ = list(dataset.keys())\n",
    "    \n",
    "    dias = get_dias(diautts_)\n",
    "    diautts_ = {dia: [diautt for diautt in diautts_ if dia in diautt] for dia in dias}\n",
    "\n",
    "    for dia, diautts in tqdm(diautts_.items()):\n",
    "        shutil.rmtree(os.path.join('data', dia), ignore_errors=True)\n",
    "        os.makedirs(os.path.join('data', dia, 'image'), exist_ok=True)\n",
    "        os.makedirs(os.path.join('data', dia, 'text'), exist_ok=True)\n",
    "        os.makedirs(os.path.join('data', dia, 'audio'), exist_ok=True)\n",
    "\n",
    "        # annots = {diautt: datasets['train'][diautt] for diautt in diautts}\n",
    "        # vids_path = {diautt: os.path.join(VIDEOS_PATH, diautt) +  '.mp4' for diautt in diautts}\n",
    "        # features_path = {diautt: os.path.join(VISUAL_FEATURES_PATH, diautt) +  '.npy' for diautt in diautts}\n",
    "        # features = {diautt: np.load(path, allow_pickle=True).item() for diautt, path in features_path.items()}\n",
    "\n",
    "        image_grmc = []\n",
    "        chat = []\n",
    "        for diautt in tqdm(diautts):\n",
    "            annot = datasets[DATASET][diautt] \n",
    "            vis = visual_features[diautt]\n",
    "            vidpath = os.path.join(VIDEOS_PATH, diautt) + '.mp4'\n",
    "\n",
    "            season = annot['Season']\n",
    "            episode = annot['Episode']\n",
    "            emotion = annot['Emotion']\n",
    "            sentiment = annot['Sentiment']\n",
    "            starttime = annot['StartTime']\n",
    "            endttime = annot['EndTime']\n",
    "            utterance = annot['Utterance']\n",
    "            speaker = annot['Speaker']\n",
    "\n",
    "            hours, minutes, seconds = starttime.split(':')\n",
    "            seconds, milliseconds = seconds.split(',')\n",
    "\n",
    "            hours, minutes, seconds, milliseconds = int(hours), int(minutes), int(seconds), int(milliseconds)\n",
    "\n",
    "            time_datetime = friends_time[season][episode] + timedelta(hours=hours, minutes=minutes, seconds=seconds)\n",
    "            time_unix = time.mktime(time_datetime.timetuple())\n",
    "            time_unix_ms = int(time_unix*1000 + milliseconds)\n",
    "\n",
    "            chat.append([speaker, utterance, time_unix_ms])\n",
    "\n",
    "            aud = diautt + '.wav'\n",
    "            !ffmpeg -i $vidpath -q:a 0 -map a data/$dia/audio/$aud\n",
    "\n",
    "            container = av.open(vidpath)\n",
    "            fps = float(container.streams.video[0].average_rate)\n",
    "            for frame in container.decode(video=0):\n",
    "                idx = frame.index\n",
    "                numpy_RGB = np.array(frame.to_image())\n",
    "                numpy_BGR = cv2.cvtColor(numpy_RGB, cv2.COLOR_RGB2BGR)\n",
    "                impath = os.path.join('data', dia, 'image', diautt + f'_frame{str(idx).zfill(5)}.jpg')\n",
    "                cv2.imwrite(impath, numpy_BGR)\n",
    "\n",
    "                features = vis[idx]\n",
    "                # Assume that there is only one unique face per frame.\n",
    "                for feat in features:\n",
    "                    to_append = {}\n",
    "\n",
    "                    age = float(feat['age'])\n",
    "                    gender = float(feat['gender'])\n",
    "                    bbox = feat['bbox']\n",
    "\n",
    "                    bbox, faceprob = [int(round(bb)) for bb in bbox[:4]], float(bbox[-1])\n",
    "\n",
    "                    embedding = feat['embedding']\n",
    "                    landmark = feat['landmark']\n",
    "\n",
    "                    if bbox[-1] < THRESHOLDS['face']:\n",
    "                        continue\n",
    "\n",
    "                    if speaker not in list(predefined_faces.keys()):\n",
    "                        continue\n",
    "\n",
    "                    embedding.reshape(1, 512)\n",
    "\n",
    "                    dists = {key: calc_angle_distance(embedding, val) for key, val \\\n",
    "                                in predefined_faces.items()}\n",
    "\n",
    "                    if dists[speaker] < THRESHOLDS['angle']:\n",
    "                        to_append['files'] = impath\n",
    "                        to_append['bbox'] = bbox\n",
    "                        to_append['faceprob'] = round(faceprob, 3)\n",
    "                        to_append['speaker'] = speaker\n",
    "                        to_append['age'] = round(age, 3)\n",
    "                        to_append['gender'] = round(gender, 3)\n",
    "\n",
    "                        image_grmc.append(to_append)\n",
    "\n",
    "        with open(os.path.join('data', dia, 'text', 'chat.csv'), 'w') as stream:\n",
    "            stream.write('speaker,utterance,time\\n')\n",
    "\n",
    "            for line in chat:\n",
    "                speaker, utterance, time_unix_ms = line\n",
    "                stream.write(speaker)\n",
    "                stream.write(',')\n",
    "                stream.write(f\"\\\"{utterance}\\\"\")\n",
    "                stream.write(',')\n",
    "                stream.write(str(time_unix_ms))\n",
    "                stream.write('\\n')    \n",
    "\n",
    "        with open(os.path.join('data', dia, 'image.json'), 'w') as stream:\n",
    "            json.dump(image_grmc, stream)"
   ]
  }
 ]
}