{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git clone https://github.com/cltl/EMISSOR --branch issue-53-processing\n",
    "cd EMISSOR\n",
    "python install.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://github.com/tae898/multimodal-datasets\n",
    "Run the dataset extraction from there, this will give\n",
    "raw-video/\n",
    "    train/ val/ test/\n",
    "the example_processing expects:\n",
    "raw-video/\n",
    "    dia01_....mp4\n",
    "raw-text/\n",
    "    dia01_....txt\n",
    "We need to match the directory structure (copy/rename/link) e.g.\n",
    "> ln -s original_dir/raw_video/train new_dir/raw_video\n",
    "Create the docker images in Taes repo:\n",
    "https://github.com/tae898/face\n",
    "Checkout the `issue-53-processing` branch from EMISSOR\n",
    "In the root directory of the EMISSOR repo run\n",
    "     python -m emissor.processing --plugin example_processing/meld --scenarios example_data/demo --dataset example_data/demo_orig  --num-jobs 4\n",
    "This runs all steps, you can specify the steps:\n",
    "    --preprocessing --init --process all\n",
    "(`all` can be replaced with the processor class name)\n",
    "* Preprocessing splits videos, copies images and text to \"scenarions\" folder\n",
    "* Init creates dia...json, image.json, text.json, without any mentions/annotations\n",
    "* Process adds mentions/annotations\n",
    "Parallelize with, max frames per second \n",
    "    --num-jobs 4  --fps_max 1\n",
    "    \n",
    "    \n",
    " python -m emissor.processing --plugin example_processing/meld --scenarios example_data/demo --dataset example_data/demo_orig  --num-jobs 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt\n",
    "#For some reason I still had to install\n",
    "#%pip install importlib-resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emissor as em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from emissor.persistence import ScenarioStorage\n",
    "from emissor.representation.annotation import AnnotationType, Token, NER\n",
    "from emissor.representation.container import Index\n",
    "from emissor.representation.scenario import Modality, ImageSignal, TextSignal, Mention, Annotation, Scenario\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import tests.test_emissor.test_interaction.driver_util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /Users/piek/Desktop/r-EMISSOR/data/myscenario3  Created \n",
      "Directory  /Users/piek/Desktop/r-EMISSOR/data/myscenario3/image  Created \n"
     ]
    }
   ],
   "source": [
    "##### Initialisation\n",
    "agent = \"Leolani2\"\n",
    "human = \"Stranger\"\n",
    "scenarioid = \"myscenario3\"\n",
    "scenario_path = \"/Users/piek/Desktop/r-EMISSOR/data\"\n",
    "imagefolder = scenario_path + \"/\" + scenarioid + \"/\" + \"image\"\n",
    "camera = cv2.VideoCapture(0)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "scenarioStorage = util.create_scenario(scenario_path, scenarioid)\n",
    "scenario = scenarioStorage.create_scenario(scenarioid, datetime.now().microsecond, datetime.now().microsecond, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ner_annotation(signal: TextSignal):\n",
    "    processor_name = \"spaCy\"\n",
    "    utterance = ''.join(signal.seq)\n",
    "\n",
    "    doc = nlp(utterance)\n",
    "\n",
    "    offsets, tokens = zip(*[(Index(signal.id, token.idx, token.idx + len(token)), Token.for_string(token.text))\n",
    "                            for token in doc])\n",
    "\n",
    "    ents = [NER.for_string(ent.label_) for ent in doc.ents]\n",
    "    entity_text = [ent.text for ent in doc.ents]\n",
    "    segments = [token.ruler for token in tokens if token.value in entity_text]\n",
    "\n",
    "    annotations = [Annotation(AnnotationType.TOKEN.name.lower(), token, processor_name, int(time.time()))\n",
    "                   for token in tokens]\n",
    "    ner_annotations = [Annotation(AnnotationType.NER.name.lower(), ent, processor_name, int(time.time()))\n",
    "                       for ent in ents]\n",
    "\n",
    "    signal.mentions.extend([Mention(str(uuid.uuid4()), [offset], [annotation])\n",
    "                            for offset, annotation in zip(offsets, annotations)])\n",
    "    signal.mentions.extend([Mention(str(uuid.uuid4()), [segment], [annotation])\n",
    "                            for segment, annotation in zip(segments, ner_annotations)])\n",
    "\n",
    "    return entity_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Hi there. Who are you Stranger?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I am Piek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stranger: I am Piek\n",
      "Leolani2: Any gossip\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " No\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leolani2: Any gossip\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " stop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##### First signals to get started\n",
    "success, frame = camera.read()\n",
    "imagepath = \"\"\n",
    "if success:\n",
    "    imagepath = imagefolder + \"/\" + str(datetime.now().microsecond) + \".png\"\n",
    "    cv2.imwrite(imagepath, frame)\n",
    "\n",
    "#### Initial prompt by the system from which we create a TextSignal and store it\n",
    "response = \"Hi there. Who are you \" + human + \"?\"\n",
    "print(agent + \": \" + response)\n",
    "textSignal = util.create_text_signal(scenario, response)\n",
    "scenario.append_signal(textSignal)\n",
    "\n",
    "utterance = input('\\n')\n",
    "print(human + \": \" + utterance)\n",
    "\n",
    "while not (utterance.lower() == 'stop' or utterance.lower() == 'bye'):\n",
    "    textSignal = util.create_text_signal(scenario, utterance)\n",
    "    # @TODO\n",
    "    ### Apply some processing to the textSignal and add annotations\n",
    "    entityText = add_ner_annotation(textSignal)\n",
    "\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    if success:\n",
    "        imageSignal = util.create_image_signal(scenario, imagepath)\n",
    "        # @TODO\n",
    "        ### Apply some processing to the imageSignal and add annotations\n",
    "        ### when done\n",
    "\n",
    "        scenario.append_signal(imageSignal)\n",
    "\n",
    "    # Create the response from the system and store this as a new signal\n",
    "\n",
    "    if not entityText:\n",
    "        utterance = \"Any gossip\" + '\\n'\n",
    "    else:\n",
    "        utterance = \"So you what do you want to talk about \" + entityText[0] + '\\n'\n",
    "\n",
    "    response = utterance[::-1]\n",
    "    print(agent + \": \" + utterance)\n",
    "    textSignal = util.create_text_signal(scenario, utterance)\n",
    "    scenario.append_signal(textSignal)\n",
    "\n",
    "    ###### Getting the next input signals\n",
    "    utterance = input('\\n')\n",
    "\n",
    "    success, frame = camera.read()\n",
    "    if success:\n",
    "        imagepath = imagefolder + \"/\" + str(datetime.now().microsecond) + \".png\"\n",
    "        cv2.imwrite(imagepath, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarioStorage.save_scenario(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
